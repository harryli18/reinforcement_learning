{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MClearning v2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"code","id":"YVUXHnGVkDPi","colab":{}},"cell_type":"code","source":["# Import the useful libraries \n","import numpy as np\n","import pandas as pd\n","import os\n","from random import randint\n","import matplotlib.pyplot as plt\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Ssv8tT92kHrK","outputId":"4c971d50-94ce-419d-e7be-4789e5cb640c","colab":{"base_uri":"https://localhost:8080/","height":202}},"cell_type":"code","source":["# Define R Matrix \n","\n","R = np.matrix([[0, 0, np.nan, np.nan, 0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n","                [0, 0, 0, np.nan, 0, 0, np.nan, np.nan, np.nan, np.nan, np.nan],\n","                [np.nan, 0, 0, 0, np.nan, np.nan, 100, np.nan, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, 0, 0, np.nan, np.nan, 100, np.nan, np.nan, np.nan, np.nan],\n","                [0, 0, np.nan, np.nan, 0, 0, np.nan, np.nan, 0, 0, np.nan],\n","                [np.nan, 0, np.nan, np.nan, 0, 0, 100, 0, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, 0, 0, np.nan, 0, 100, 0, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, np.nan, np.nan, np.nan, 0, 100, 0, np.nan, np.nan, 0],\n","                [np.nan, np.nan, np.nan, np.nan, 0, np.nan, np.nan, np.nan, 0, 0, np.nan],\n","                [np.nan, np.nan, np.nan, np.nan, 0, np.nan, np.nan, np.nan, 0, 0, 0],\n","                [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0, np.nan, 0, 0]])\n","\n","print(R)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[  0.   0.  nan  nan   0.  nan  nan  nan  nan  nan  nan]\n"," [  0.   0.   0.  nan   0.   0.  nan  nan  nan  nan  nan]\n"," [ nan   0.   0.   0.  nan  nan 100.  nan  nan  nan  nan]\n"," [ nan  nan   0.   0.  nan  nan 100.  nan  nan  nan  nan]\n"," [  0.   0.  nan  nan   0.   0.  nan  nan   0.   0.  nan]\n"," [ nan   0.  nan  nan   0.   0. 100.   0.  nan  nan  nan]\n"," [ nan  nan   0.   0.  nan   0. 100.   0.  nan  nan  nan]\n"," [ nan  nan  nan  nan  nan   0. 100.   0.  nan  nan   0.]\n"," [ nan  nan  nan  nan   0.  nan  nan  nan   0.   0.  nan]\n"," [ nan  nan  nan  nan   0.  nan  nan  nan   0.   0.   0.]\n"," [ nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.]]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"W5ipMz4mkJDO","outputId":"a107ed20-593f-469e-add8-e4615192b572","colab":{"base_uri":"https://localhost:8080/","height":202}},"cell_type":"code","source":["# Initialise Q Matrix \n","Q = np.matrix(np.zeros([11,11]))\n","\n","print(Q)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"YBe40c8ukKKa","colab":{}},"cell_type":"code","source":["def q_learning(alpha=0.5, episodes=10):\n","    # initialise current state in random\n","    current_state = randint(0, 10)\n","      # check available actions in the current state\n","    current_state_row = R[current_state,]\n","    av_act = np.where(current_state_row >= 0)[1]\n","  \n","    # randomly select the next action and record in steps\n","    next_action = int(np.random.choice(av_act,1))\n","    \n","    for _ in range(episodes):\n","        G = 0 # Initialize G to store cumulative reward for the current iteration\n","        if current_state == 6:\n","            reward = reward + np.max(Q[current_state, :])\n","        else:\n","            while current_state != 6:\n","                    current_state_row = R[current_state,]\n","                    av_act = np.where(current_state_row >= 0)[1]\n","                    # randomly select the next action and record in steps\n","                    next_action = int(np.random.choice(av_act,1))\n","                    # check available actions, particular for early episode that Q value is 0 \n","                    current_state_row = R[current_state, :]\n","                    av_act = np.where(current_state_row >= 0)[1]\n","                    G += R[current_state, next_action]\n","                    current_state = next_action\n","            Q[current_state, next_action] = Q[current_state, next_action] + alpha * (G - Q[current_state, next_action])\n","\n","  # return Q-matrix and number of steps\n","    return Q, alpha, episodes"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"s9sHb70rkLyl","colab":{}},"cell_type":"code","source":["def evaluate(Q, episode, alpha, gamma, epsilon, decay):\n","  \n","  # initiate current state\n","#   current_state = randint(0,10)\n","  current_state = 8\n","  steps = [current_state]\n","  reward = 0\n","  \n","  # set rule if initiated from Station 7\n","  if current_state == 6:\n","    reward += np.max(Q[current_state, :])\n","  \n","  # step for start from other stations\n","  else:\n","    while current_state != 6:\n","      \n","      # check available actions, particular for early episode that Q value is 0 \n","      current_state_row = R[current_state, :]\n","      av_act = np.where(current_state_row >= 0)[1]\n","      \n","      # when Q-matrix is 0, select randomly from available actions\n","      if Q[current_state, av_act].sum() == 0:\n","        next_step = int(np.random.choice(av_act, size=1))\n","      \n","      else:\n","      # index for maximum value\n","        max_index = np.where(Q[current_state,:] == np.max(Q[current_state, av_act]))[1]\n","\n","        if max_index.shape[0] > 1:\n","          next_step = int(np.random.choice(max_index, size = 1))\n","        else:\n","          next_step = int(max_index)\n","      \n","      # append next action and reward\n","      next_reward = np.max(Q[current_state, next_step])\n","      \n","      steps.append(next_step)\n","      reward += next_reward\n","      current_state = next_step\n","      \n","  \n","  average_reward.append((episode, alpha, gamma, epsilon, decay, reward/len(steps)))  \n","  step_cont.append((episode, alpha, gamma, epsilon, decay, len(steps)))\n","  \n","  return average_reward, step_cont"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"fq6l8b2-kWS1","colab":{}},"cell_type":"code","source":["# Define initial parameters \n","alphas = [0.5] # alpha is the learning rate\n","gammas = [0.8] # gamma is the discount factor \n","episodes = [10] # epsilon is the exploration factor\n","decays = [0.1] # df is the decay factor for epsilon"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T6tcIIVSYe4D","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"aG7hLNwAkN6N","outputId":"475ac2c0-ba31-4509-e80a-80cc89963fae","colab":{"base_uri":"https://localhost:8080/","height":269}},"cell_type":"code","source":["average_reward = []\n","step_cont = []\n","\n","start = time.time()\n","\n","for alpha in alphas:\n","      for episodes in range(0,10):\n","          Q, alpha, episodes, = q_learning(alpha, episodes)\n","    \n","          if np.matrix.max(Q) > 0:\n","            Q_norm = (Q/np.matrix.max(Q)*100)\n","          else:\n","            Q_norm = Q\n","\n","          average_reward, step_cont = evaluate(Q_norm, episode, alpha, gamma, epsilon, decay)\n","      \n","      # Reset the Q-matrix and epsilon to initial value\n","      Q = np.matrix(np.zeros([11,11]))\n","      epsilon = epsilon\n","          \n","end = time.time()\n","                  \n","print(Q_norm.astype(int))\n","# print(Q_norm.round(decimals=5))\n","# print(average_reward)\n","# print(step_cont)\n","print('Time used :', end-start, 'seconds')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater_equal\n","  \n","/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in greater_equal\n","/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater_equal\n","/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in greater_equal\n"],"name":"stderr"},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'reward' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-2b41d2367d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-fe629dae6658>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(alpha, episodes)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Initialize G to store cumulative reward for the current iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'reward' referenced before assignment"]}]},{"metadata":{"colab_type":"code","id":"YBPXuH1akUnY","colab":{}},"cell_type":"code","source":["df_reward = pd.DataFrame.from_records(average_reward, columns = ['episode', 'alpha', 'gamma', 'epsilon', 'decay', 'Average Reward Per Step'])\n","df_step = pd.DataFrame.from_records(step_cont, columns = ['episode', 'alpha', 'gamma', 'epsilon', 'decay','Steps'])\n","\n","df = pd.merge(df_reward, df_step, on=['episode', 'alpha', 'gamma', 'epsilon', 'decay'])\n","print(df.shape)\n","df.head(5)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"9v3pbd7kkjFE","colab":{}},"cell_type":"code","source":["# Plot the performance chart\n","\n","plt.style.use('seaborn')\n","\n","fig, ax1 = plt.subplots()\n","\n","fig.suptitle('PERFORMANCE VS EPISODE', fontsize=18, y=1.05, fontweight='bold')\n","\n","ax2 = ax1.twinx()\n","ax1.plot(df['episode'], df['Steps'], color='#00CF38', linewidth=1, alpha=2)\n","ax2.plot(df['episode'], df['Average Reward Per Step'], color='#0767FF', linewidth=2, alpha=0.7)\n","\n","ax1.set_ylim(0, 40)\n","ax2.set_ylim(0, 80)\n","\n","ax1.set_xlabel('EPISODES', color='#979797', fontsize='large', fontweight='bold')\n","ax1.set_ylabel('STEPS', color='#00CF38', fontsize='large', fontweight='bold', labelpad=10)\n","ax2.set_ylabel('AVERAGE REWARD PER STEP', color='#0767FF', fontsize='large', fontweight='bold', labelpad=10)\n","\n","plt.text(650, 75, r'$\\alpha=0.9$, $\\gamma=0.8$, $\\epsilon=0.1$, df=0.95')\n","\n","fig.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"qXzAgsoVlx6H","colab":{}},"cell_type":"code","source":["# Path from station 9\n","\n","current_state = 8\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"75MS5QvlLbdy","colab":{}},"cell_type":"code","source":["# Path from station 10\n","\n","current_state = 9\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"hVTIPKiYLgYY","colab":{}},"cell_type":"code","source":["# Path from station 11\n","\n","current_state = 10\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"bCvSpALNLuQW","colab":{}},"cell_type":"code","source":["# Path from station 5\n","\n","current_state = 4\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"cBZVPbkFL0e7","colab":{}},"cell_type":"code","source":["# Path from station 2\n","\n","current_state = 1\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"sbxt0VrWL3RJ","colab":{}},"cell_type":"code","source":["# Path from station 1\n","\n","current_state = 0\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"-hY7aNqBL96I","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}