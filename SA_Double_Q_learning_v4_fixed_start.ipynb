{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SA_Double_Q_learning_v4_fixed_start.ipynb","version":"0.3.2","provenance":[{"file_id":"1t3pMoIQMRYV-kqbAK9KzHFJgkqubV7U2","timestamp":1552991080114}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"YVUXHnGVkDPi","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import the useful libraries \n","import numpy as np\n","import pandas as pd\n","import os\n","from random import randint\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ssv8tT92kHrK","colab_type":"code","outputId":"b4d35747-9bd7-4443-87c7-93932f947c69","executionInfo":{"status":"ok","timestamp":1553165604990,"user_tz":0,"elapsed":1420,"user":{"displayName":"Harry li","photoUrl":"https://lh5.googleusercontent.com/-uHMvrRieOmg/AAAAAAAAAAI/AAAAAAAAJh4/38v3PXTZ5NI/s64/photo.jpg","userId":"15495712785475825794"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["# Define R Matrix \n","\n","R = np.matrix([[0, 0, np.nan, np.nan, 0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n","                [0, 0, 0, np.nan, 0, 0, np.nan, np.nan, np.nan, np.nan, np.nan],\n","                [np.nan, 0, 0, 0, np.nan, np.nan, 100, np.nan, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, 0, 0, np.nan, np.nan, 100, np.nan, np.nan, np.nan, np.nan],\n","                [0, 0, np.nan, np.nan, 0, 0, np.nan, np.nan, 0, 0, np.nan],\n","                [np.nan, 0, np.nan, np.nan, 0, 0, 100, 0, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, 0, 0, np.nan, 0, 100, 0, np.nan, np.nan, np.nan],\n","                [np.nan, np.nan, np.nan, np.nan, np.nan, 0, 100, 0, np.nan, np.nan, 0],\n","                [np.nan, np.nan, np.nan, np.nan, 0, np.nan, np.nan, np.nan, 0, 0, np.nan],\n","                [np.nan, np.nan, np.nan, np.nan, 0, np.nan, np.nan, np.nan, 0, 0, 0],\n","                [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0, np.nan, 0, 0]])\n","\n","print(R)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[  0.   0.  nan  nan   0.  nan  nan  nan  nan  nan  nan]\n"," [  0.   0.   0.  nan   0.   0.  nan  nan  nan  nan  nan]\n"," [ nan   0.   0.   0.  nan  nan 100.  nan  nan  nan  nan]\n"," [ nan  nan   0.   0.  nan  nan 100.  nan  nan  nan  nan]\n"," [  0.   0.  nan  nan   0.   0.  nan  nan   0.   0.  nan]\n"," [ nan   0.  nan  nan   0.   0. 100.   0.  nan  nan  nan]\n"," [ nan  nan   0.   0.  nan   0. 100.   0.  nan  nan  nan]\n"," [ nan  nan  nan  nan  nan   0. 100.   0.  nan  nan   0.]\n"," [ nan  nan  nan  nan   0.  nan  nan  nan   0.   0.  nan]\n"," [ nan  nan  nan  nan   0.  nan  nan  nan   0.   0.   0.]\n"," [ nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"OudwAxa20rxS","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"W5ipMz4mkJDO","colab_type":"code","outputId":"44ae4123-26e4-4f21-9a27-bc8efd437232","executionInfo":{"status":"ok","timestamp":1553165609155,"user_tz":0,"elapsed":1200,"user":{"displayName":"Harry li","photoUrl":"https://lh5.googleusercontent.com/-uHMvrRieOmg/AAAAAAAAAAI/AAAAAAAAJh4/38v3PXTZ5NI/s64/photo.jpg","userId":"15495712785475825794"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["# Initialise Q Matrix \n","Q1 = Q2 = np.matrix(np.zeros([11,11]))\n","\n","print(Q2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"6oWeM76zmXf5","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","def maxAction(Q1,Q2,state):\n","  values = np.array([Q1[state,a] + Q2[state,a] for a in range(12)])\n","  action = np.argmax(values)\n","    \n","  return action"],"execution_count":0,"outputs":[]},{"metadata":{"id":"efdN2P2SR98_","colab_type":"code","colab":{}},"cell_type":"code","source":["maxAction(Q1,Q2,1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YBe40c8ukKKa","colab_type":"code","colab":{}},"cell_type":"code","source":["def q_learning(alpha, gamma, epsilon, decay):\n","    \n","  # initialise current state in random\n","  current_state = randint(0, 10)\n","  \n","  # check available actions in the current state\n","  current_state_row = R[current_state,]\n","  av_act = np.where(current_state_row >= 0)[1]\n","  \n","  # randomly select the next action and record in steps\n","  next_action = int(np.random.choice(av_act,1))\n","  \n","  # epislon greedy policy\n","  if np.random.uniform(0, 1) > epsilon:\n","    one_step_ahead_q_value = maxAction(Q1,Q2,current_state)\n","  else:\n","    one_step_ahead_q_value = np.random.choice(np.array(Q[next_action,:]).ravel())\n","    \n","  # update Double Q-matrix \n","  rand = np.random.random()\n","  if rand > 0.5:\n","    Q1[current_state, next_action] = Q1[current_state, next_action] + alpha * (R[current_state, next_action] + gamma * one_step_ahead_q_value - Q1[current_state, next_action])\n","  else:\n","    Q2[current_state, next_action] = Q2[current_state, next_action] + alpha * (R[current_state, next_action] + gamma * one_step_ahead_q_value - Q2[current_state, next_action])\n","\n","  # update epsilon value\n","  epsilon *= decay\n","  \n","  # return Q-matrix and number of steps\n","  return Q1, Q2, alpha, gamma, epsilon, decay"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s9sHb70rkLyl","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate(Q1, Q2, episode, alpha, gamma, epsilon, decay):\n","  \n","  # initiate current state\n","#   current_state = randint(0,10)\n","  current_state = 8\n","  steps = [current_state]\n","  reward = 0\n","  Q = Q1 + Q2 \n","  # set rule if initiated from Station 7\n","  if current_state == 6:\n","    reward += np.max(Q[current_state, :])\n","  \n","  # step for start from other stations\n","  else:\n","    while current_state != 6:\n","      \n","      # check available actions, particular for early episode that Q value is 0 \n","      current_state_row = R[current_state, :]\n","      av_act = np.where(current_state_row >= 0)[1]\n","      \n","      # when Q-matrix is 0, select randomly from available actions\n","      if Q[current_state, av_act].sum() == 0:\n","        next_step = int(np.random.choice(av_act, size=1))\n","      \n","      else:\n","      # index for maximum value\n","        max_index = np.where(Q[current_state,:] == np.max(Q[current_state, av_act]))[1]\n","\n","        if max_index.shape[0] > 1:\n","          next_step = int(np.random.choice(max_index, size = 1))\n","        else:\n","          next_step = int(max_index)\n","      \n","      # append next action and reward\n","      next_reward = np.max(Q[current_state, next_step])\n","      \n","      steps.append(next_step)\n","      reward += next_reward\n","      current_state = next_step\n","      \n","  \n","  average_reward.append((episode, alpha, gamma, epsilon, decay, reward/len(steps)))  \n","  step_cont.append((episode, alpha, gamma, epsilon, decay, len(steps)))\n","  \n","  return average_reward, step_cont"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fq6l8b2-kWS1","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define initial parameters \n","alphas = [0.5] # alpha is the learning rate\n","gammas = [0.8] # gamma is the discount factor \n","epsilons = [0.1] # epsilon is the exploration factor\n","decays = [0.1] # df is the decay factor for epsilon"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aG7hLNwAkN6N","colab_type":"code","outputId":"4b3dc6fc-5aa3-4d83-920d-ad896a9e63f0","colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["average_reward = []\n","step_cont = []\n","\n","start = time.time()\n","\n","for alpha in alphas:\n","  for gamma in gammas:\n","    for decay in decays:\n","      for epsilon in epsilons:\n","        for episode in range(1000):\n","\n","          Q1, Q2, alpha, gamma, epsilon, decay = q_learning(alpha, gamma, epsilon, decay)\n","    \n","          if np.matrix.max(Q1) > 0:\n","            Q1_norm = (Q1/np.matrix.max(Q1)*100)\n","\n","          else:\n","            Q1_norm = Q1\n","\n","          if np.matrix.max(Q2) > 0:\n","            Q2_norm = (Q2/np.matrix.max(Q1)*100)\n","\n","          else:\n","            Q2_norm = Q2\n","            \n","          average_reward, step_cont = evaluate(Q1_norm, Q2_norm, episode, alpha, gamma, epsilon, decay)\n","      \n","      # Reset the Q-matrix and epsilon to initial value\n","      Q1, Q2 = np.matrix(np.zeros([11,11]))\n","      epsilon = epsilon\n","          \n","end = time.time()\n","                  \n","print(Q_norm.astype(int))\n","# print(Q_norm.round(decimals=5))\n","# print(average_reward)\n","# print(step_cont)\n","print('Time used :', end-start, 'seconds')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in greater_equal\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in greater_equal\n"],"name":"stderr"}]},{"metadata":{"id":"YBPXuH1akUnY","colab_type":"code","colab":{}},"cell_type":"code","source":["df_reward = pd.DataFrame.from_records(average_reward, columns = ['episode', 'alpha', 'gamma', 'epsilon', 'decay', 'Average Reward Per Step'])\n","df_step = pd.DataFrame.from_records(step_cont, columns = ['episode', 'alpha', 'gamma', 'epsilon', 'decay','Steps'])\n","\n","df = pd.merge(df_reward, df_step, on=['episode', 'alpha', 'gamma', 'epsilon', 'decay'])\n","print(df.shape)\n","df.head(5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9v3pbd7kkjFE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Plot the performance chart\n","\n","plt.style.use('seaborn')\n","\n","fig, ax1 = plt.subplots()\n","\n","fig.suptitle('PERFORMANCE VS EPISODE', fontsize=18, y=1.05, fontweight='bold')\n","\n","ax2 = ax1.twinx()\n","ax1.plot(df['episode'], df['Steps'], color='#00CF38', linewidth=1, alpha=2)\n","ax2.plot(df['episode'], df['Average Reward Per Step'], color='#0767FF', linewidth=2, alpha=0.7)\n","\n","ax1.set_ylim(0, 40)\n","ax2.set_ylim(0, 80)\n","\n","ax1.set_xlabel('EPISODES', color='#979797', fontsize='large', fontweight='bold')\n","ax1.set_ylabel('STEPS', color='#00CF38', fontsize='large', fontweight='bold', labelpad=10)\n","ax2.set_ylabel('AVERAGE REWARD PER STEP', color='#0767FF', fontsize='large', fontweight='bold', labelpad=10)\n","\n","plt.text(650, 75, r'$\\alpha=0.9$, $\\gamma=0.8$, $\\epsilon=0.1$, df=0.95')\n","\n","fig.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qXzAgsoVlx6H","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 9\n","\n","current_state = 8\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"75MS5QvlLbdy","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 10\n","\n","current_state = 9\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hVTIPKiYLgYY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 11\n","\n","current_state = 10\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bCvSpALNLuQW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 5\n","\n","current_state = 4\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cBZVPbkFL0e7","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 2\n","\n","current_state = 1\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sbxt0VrWL3RJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Path from station 1\n","\n","current_state = 0\n","step = [current_state+1]\n","reward = 0\n","\n","if current_state == 6:\n","  reward += np.max(Q_norm[current_state, :])\n","  \n","else:\n","\n","  while current_state != 6:\n","    next_step = np.argmax(Q_norm[current_state, :])\n","    next_reward = np.max(Q_norm[current_state, :])\n","    step.append(next_step+1)\n","    reward += next_reward\n","    current_state = next_step\n","\n","print(\"Selected path:\")\n","print(step)\n","\n","print('Steps:')\n","print(len(step))\n","\n","print(\"Rewards:\")\n","print(int(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-hY7aNqBL96I","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}